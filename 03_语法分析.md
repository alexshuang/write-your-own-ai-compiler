语法分析（Syntax Analysis）是编译流程中的一个关键阶段，主要任务是检查源代码是否符合预期的语法规则，并生成相应的抽象语法树（Abstract Syntax Tree, AST）。

例如，代码 **`def func(): print('Hello World!')`** 经过语法分析器（Parser）处理后得到下列语法树：根节点 `Module` 表示程序，`Block` 是它的 body 节点，承载该程序的所有语句。`FunctionDef` 节点表示函数定义，`Signature` 和 `Block` 节点表示函数签名和函数体。`FunctionCall` 节点表示函数调用语句。

```
Module:
  Block:
    Function define func
      Signature
        []
      Block:
        Function Call print, arg_list: ['<Arg 0 = "Hello World!">']  (resolved)
```

虽然我们说，编译过程先是词法分析，其次语法分析，接着语义分析，但实际上词法分析和语法分析是交替进行的。就像词法分析会根据前缀字符（>=1个起始字符）来判断要解析哪种类型的 token 一样，语法分析也会根据起始 token 来决定将后续 token 解析成哪种类型的树节点。

```
class Parser:
    ...
    def parse_statement(self):
        t = self.tokenizer.peak()
        ...
        if t.data == "def":
            ret = self.parse_function_decl()
        elif t.data == "return":
            ret = self.parse_return()
        elif t.data == "import":
            ret = self.parse_import()
        elif t.data == "if":
```

fleet-compiler [语法分析器（Parser）](https://github.com/alexshuang/fleet-compiler/blob/main/fleet_compiler/frontend/parsing.py)（`fleet_compiler/frontend/parsing.py`）通过 `self.tokenizer.peak()` 预取下一个 token，如果它是 "def"，就调用 `parse_function_decl()` 来解析创建 FunctionDef 节点。

```
class AstNode(ABC):
    @abstractmethod
    def accept(self, visitor): ...

class Statement(AstNode): ...

class FunctionDef(Statement):
    def __init__(self, name: str, signautre: Signature, block: Block) -> None:
        super().__init__()
        self.name = name
        self.signature = signautre
        self.block = block
    
    def accept(self, visitor):
        return visitor.visitFunctionDef(self)
```

# 语法规则

我们知道每种编程语言都有它的语法规则，比如 python 用缩进来区分作用域，用 "def" 作为函数定义的关键字等。**parse_function_decl()** 也需要遵循相应的规则，将 tokens 解析成 FunctionDef 节点。

```
    funcitonDef = 'def' Identifier signature ':' terminator
    signature = '(' parameterDef? (',' parameterDef)? ')' (-> Identifier)?
    parameterDef = Identifier ('=' expressionStatement)?
    terminator = '\n' | ';'
    block = statementList
    statementList = statement*
```
`funcitonDef` 表示函数定义的语法规则：字符串 "def" 后跟着函数名标识符，接着是函数签名，然后是冒号，最后是终结符，也就是 '\n'。依据该规则，`parse_function_decl()` 就可以从字符串 `def func(): print('Hello World!')` 中解析出 FunctionDef 节点：

```
    def parse_function_decl(self):
        self.tokenizer.next() # skip def
        t = self.tokenizer.peak()
        if t.kind == TokenKind.Identifier:
            func_name = self.tokenizer.next().data # skip identifier
            t = self.tokenizer.peak()
            if t.data == '(':
                signature = self.parse_signature()
                t = self.tokenizer.peak()
                if t.data == ':':
                    self.tokenizer.next() # skip :
                    self.skip_terminator() # skip nl
                    self.enter_indent()
                    func_body = self.parse_block()
                else:
                    self.raise_func_decl_error(f"Expect got ':' here, not {t.data}")
            else:
                self.raise_func_decl_error(f"Expect got '(' here, not {t.data}")
        else:
            self.raise_func_decl_error(f"Expect got identifier here, not {t.data}")
        return FunctionDef(func_name, signature, func_body)
```

# 不完整支持 Python

目前，fleet-compiler 支持的模型 [gpt2.py](https://github.com/alexshuang/fleet-compiler/blob/main/examples/gpt2.py) ，它修改自 [GPT in 60 Lines of NumPy](https://github.com/jaymody/picoGPT/blob/29e78cc52b58ed2c1c483ffea2eb46ff6bdec785/gpt2_pico.py#L3-L58)。如果你比较两者的话，会看到它们还是有区别的，例如：
- 不支持类和对象。example 里所有对象操作都换成了函数操作，例如 `a.transpose()` -> `np.transpose(a)`。
- 不支持 for loop。原先的 for loop 都被展开了。
- 不支持闭包，函数中使用的变量要么在函数内定义，要么通过参数传进来。其实 AST 解释器是支持闭包的，但由于 MLIR 不支持，简单起见就先把 gpt2.py 改成纯函数的形式。

上述 python 特性花些时间是可以支持的，但它们不是跑通 gpt2.py 的必备特性，因此现阶段暂时先这样。

---
